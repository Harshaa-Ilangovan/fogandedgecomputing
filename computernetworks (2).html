
<!DOCTYPE html>
<html lang="en" scroll-behavior="smooth">

<head>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="mouse (2).css">
    <link rel="stylesheet" href="cn (2).css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.1/css/fontawesome.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat&display=swap"rel="stylesheet"/>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FOG EDGE</title>
</head>
<body>
<div id="sectionmain">
    <div class="back">
      <nav class="navbar navbar-expand-lg navbar-light backkkk ">
        <a class="navbar-brand " href="#">
          <img src="C:\Users\DELL\OneDrive\Desktop\code\image\logo.jpg" class="navbar-image" alt="" />
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon "></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
            <div class="navbar-nav ml-auto">
                <a class="nav-link active" href="homee (2).html" style="color:white;">
                    Home <span class="sr-only">(current)</span>
                </a>
                <a class="nav-link white" href="computernetworks (2).html" style="color:white;">Modules</a>
                <a class="nav-link white" href="about (2).html" style="color:white;">About Us</a>
                
                <button class="col-3 mt-2 mr-2 ml-2 mb-2 button" onclick="document.getElementById('id01').style.display='block'" style="width:auto;">Login</button>
                <button class="button1 button col-3 mt-2 mr-2 ml-2 mb-2 button" onclick="document.getElementById('id02').style.display='block'" style="width:auto;"  >Sign Up</button>

                
            </div>
        </div>
    </nav>
      <div class=" d-flex flex-row pad">
        <div>
          <h1 class="head col-6 col-sm-6 col-md-6 col-lg-6 col-xl-6">FOG EDGE COMPUTING</h1>
          <p class=" col-sm-6 col-md-6 col-lg-6 col-xl-6">Fog computing is a term introduced by Cisco in Jan 2014 that refers to extending cloud computing to the edge of an enterprise’s network</p>
          <button onclick="document.getElementById('id01').style.display='block'" style="width:auto;"  class="col-5 mt-5 mr-5 ml-5 mb-5 button">Login</button>
          <button class="btn btn-primary button col-5 mt-5 mr-5 ml-5 mb-5" onclick="display('sectionid03')">GO TO LEARNINGS</button>
        </div>
        
       
        <div class="container img col-6 col-sm-6 col-md-6 col-lg-6 col-xl-6">
          
          <div class="middle">
            <div class="text">Start Learning</div>
          </div>
        </div>
       </div>
      </div>
       
    <div class="d-flex flex-row pading bak">
      <a class="h-btn btn-primary-dark">Contact Us</a>
          <div class="d-flex flex-row">
              <div class="d-flex flex-column">
                  <img src="image/phone.png" class="icon"/>
                  <img src="image/linkedin.png" class="icon"/>
                  <img src="image/inst.jpg" class="icon"/>
              </div>
              <div class="padding white">
                  <p>91 9677545420</p>
                  <p>Linkedin</p>
                  <p>Instagram</p>
              </div>
              <div class="vl"></div>
              <a class="h-btn btn-primary-dark">RESOURCES</a>
          <div class="d-flex flex-row">
              <div class="d-flex flex-column">
                  <img src="image/comp.png" class="icon"/>
                  <img src="image/file.png" class="icon"/>
                  <img src="image/comp.png" class="icon"/>
              </div>
              <div class="padding white">
                  <p>google</p>
                  <p>IEEE articles</p>
                  <p>Books</p>
              </div>
              <div class="vl"></div>
              <a class="h-btn btn-primary-dark">Editors</a>
          <div class="d-flex flex-row">
              <div class="d-flex flex-column">
                  <img src="image/girl1.png" class="icon"/>
                  <img src="image/girl2.png" class="icon"/>
                  
              </div>
              <div class="padding white">
                  <p>Harshaa</p>
                  <p>Atchaya</p>
                  
              </div>    
          </div>        
                      
              
              
          </div>
          
      </div>
      
  
        </div>
</div>
<div id="id01" class="modal">
          
          <form class="modal-content animate" action="computernetworks (2).html" method="post">
            <div class="imgcontainer">
              <span onclick="document.getElementById('id01').style.display='none'" class="close" title="Close Modal">&times;</span>
              <img src="image/human.png" alt="Avatar" class="avatar">
            </div>
        
            <div class="container">
              <label for="uname"><b>Username</b></label>
              <input type="text" placeholder="Enter Username" name="uname" required>
        
              <label for="psw"><b>Password</b></label>
              <input type="password" placeholder="Enter Password" name="psw" required>
                
              <a href="computernetworks (2).html" style="color: white;"><button type="submit"  type="button" value="Redirect Me" onclick="document.getElementById('id01').style.display='none'">Login</button></a>
              
              <label>
                <input type="checkbox" checked="checked" name="remember"> Remember me
              </label>
            </div>
        
            <div class="container" style="background-color:#aacbcd">
              <button type="button" onclick="document.getElementById('id01').style.display='none'" class="cancelbtn">Cancel</button>
              <span class="psw">Forgot <a href="#">password?</a></span>
            </div>
          </form>
</div>
<div id="id02" class="modal">
  <span onclick="document.getElementById('id02').style.display='none'" class="close" title="Close Modal">&times;</span>
  <img src="image/human.png" alt="Avatar" class="avatar">
            
  <form class="modal-content" action="computernetworks (2).html">
    <div class="container">
      <h1>Sign Up</h1>
      <p>Please fill in this form to create an account.</p>
      <hr>
      <label for="email"><b>Email</b></label>
      <input type="text" placeholder="Enter Email" name="email" required>

      <label for="psw"><b>Password</b></label>
      <input type="password" placeholder="Enter Password" name="psw" required>

      <label for="psw-repeat"><b>Repeat Password</b></label>
      <input type="password" placeholder="Repeat Password" name="psw-repeat" required>

      <label>
        <input type="checkbox" checked="checked" name="remember" style="margin-bottom:15px"> Remember me
      </label>

      <p>By creating an account you agree to our <a href="#" style="color:dodgerblue">Terms & Privacy</a>.</p>

      <div class="clearfix">
        <button type="button" onclick="document.getElementById('id02').style.display='none'" class="cancelbtn">Cancel</button>
        <a href="computernetworks (2).html" style="color: white;"><button type="submit"  type="button" value="Redirect Me" onclick="document.getElementById('id02').style.display='none'">Sign Up</button></a>
      </div>
    </div>
  </form>
</div>
<div id="sectionid03" class="modal">
      <h1 style="text-align: center;">CONTENTS TO BE COVERED</h1>
      <div style="background-color: rgb(165, 102, 154); height:300vh;">
        <div class="d-flex flex-column">
          <div class="d-flex flex-row">
         <div class="card">
          <h3>Introduction</h3>
         <img src="image/introbg.jpg" width="500" height="504" frameborder="0" title="INTRODUCTION"/>
            <button class="btn btn-danger button" onclick="display('sectionintro')">start</button>
            
        </div>
         <div class="card">
          <h3>Architecture</h3>
          <img src="image/archibg.jpg" width="500" height="404" frameborder="0" title="Architecture" webkitallowfullscreen mozallowfullscreen allowfullscreen/>
          <button class="btn btn-danger button" onclick="display('sectionarchi')">start</button>
         </div>
        </div>
         <div class="d-flex flex-row">
         <div class="card">
          <h3>Algorithms</h3>
          <img src="image/algbg.jpg" width="500" height="504" frameborder="0" title="algorithms" webkitallowfullscreen mozallowfullscreen allowfullscreen/>
          <button class="btn btn-danger button" onclick="display('sectionalg')">start</button>
         </div>
         <div class="card">
          <h3>Advantages</h3>
          <img src="image/advantage.jpg" width="500" height="504" frameborder="0" title="Advantage" webkitallowfullscreen mozallowfullscreen allowfullscreen/>
          <button class="btn btn-danger button" onclick="display('sectionadvan')">start</button>
         </div>
         
        </div>
        <div class="d-flex flex-row">
          <div class="card">
            <h3>Disadvantage</h3>
            <img src="image/disadvantage.jpg" width="500" height="504" frameborder="0" title="Disabvantage" webkitallowfullscreen mozallowfullscreen allowfullscreen/>
            <button class="btn btn-danger button" onclick="replace('intro.html')">start</button>
           </div>
           <div class="card">
            <h3>How to overcome the disadvantage</h3>
            <img src="image/howto.jpg" width="500" height="504" frameborder="0" title="conclusion" webkitallowfullscreen mozallowfullscreen allowfullscreen/>
            <button class="btn btn-danger button" onclick="display('sectionover')">start</button>
           </div>
        </div>
          </div>

          </div>

          <button class="btn btn-secondary button " onclick="display('sectionmain')">Previous</button>
</div>
<div id="sectionintro" class="modal">
          <div  style="margin: 20px; padding:15px;">
            <h1 class="heading" style="color:rgb(255, 255, 255); background-color:black;height:8vh;width:auto;">INTRODUCTION TO FOG AND EDGE COMPUTING</h1>
            <div class="card1">
          
          
            <h2 style="color: rgb(255, 64, 0);">INTRODUCTION</h2>
            <p class="p1">Accessing remote computing resources offered by cloud data centers has become the <b>de facto</b> model for most Internet-based applications. Typically, data generated by user devices such as smartphones and wearables, or sensors in a smart city or factory are all transferred to geographically distant clouds to be processed and stored. This computing model is not practical for the future, because it is likely to increase <b>communication latencies</b> when billions of devices are connected to the Internet.Applications will be adversely impacted because of the increase in communication latencies, thereby degrading the overall <b>Quality-of-Service (QoS) and Quality-of-Experience (QoE)</b> .An alternative computing model that can alleviate the above problem is bringing computing resources closer to user devices and sensors, and using them for data processing (even if only partial) . This would reduce the amount of data sent to the cloud, consequently reducing communication latencies. To realize this computing model, the current research trend is to decentralize some of the computing resources available in large data centers by distributing them towards the edge of the network closer to the end-users and sensors. These resources may take the form of either 
            </p>
              <p class="p2">(i) dedicated “micro” data centers that are conveniently and safely located within public/private infrastructure </p>
              <p class="p2"> (ii) Internet nodes, such as routers, gateways, and switches that are augmented with computing capabilities. </p>
              <p class="p1">A computing model that makes use of resources located at the edge of the network is referred to as “edge computing” . A model that makes use of both edge resources and the cloud is referred to as “fog computing”.
                Contrary to cloud resources, the resources at the edge are:  </p>
                <p class="p2">(i) resource constrained—limited computational resources, because edge devices have smaller processors and a limited power budget,</p>
                 <p  class="p2"> (ii) heterogeneous—processors with different architectures, and </p>
                  <p  class="p2">(iii) dynamic—their workloads change, and applications compete for the limited resources. </p>
                  <p  class="p1">Therefore, managing resources is one of the key challenges in fog and edge computing. The focus of this article is to review the architectures, infrastructure, and algorithms that underpin resource management in fog/edge computing.</p>
                </div>
                <div class="card2">
                <img src="image/intro1.png"alt="Screenshot-2023-08-29-205010" border="10"/></div>
                </div>
                <div class="card4">
                  
                  <p class="p1"><b> Fog computing</b></p>
                  
                 <p class="p1"> Fog computing bridges the gap between the cloud and end devices
                  (e.g., IoT nodes) by enabling computing, storage, networking, and data
                  management on network nodes within the close vicinity of IoT devices.
                  Consequentially, computation, storage, networking, decision makingand data management not only occur in the cloud, but also occur along
                  the IoT-to-Cloud path as data traverses to the cloud (preferably close to
                  the IoT devices). For instance, compressing the GPS data can happen at
                  the edge before transmission to the cloud in Intelligent Transportation
                  Systems (ITS) . Fog computing is defined by the OpenFog Consor-
                  tium  as “a horizontal system-level architecture that distributes com-
                  puting, storage, control and networking functions closer to the users
                  along a cloud-to-thing continuum.” The “horizontal” platform in fog
                  computing allows computing functions to be distributed between dif-
                  ferent platforms and industries, whereas a vertical platform promotes
                  siloed applications. A vertical platform may provide strong sup-
                  port for a single type of application (silo), but it does not account for
                  platform-to-platform interaction in other vertically focused platforms.
                  In addition to facilitating a horizontal architecture, fog computing pro-
                  vides a flexible platform to meet the data-driven needs of operators and
                  users. Fog computing is intended to provide strong support for the In-
                  ternet of Things.</p>
                  <p class="p1"><b>Fog vs. cloud</b></p>
                  <p class="p1"> 
                  A common example that is often used to distinguished fog and cloud
                  computing is whether latency-sensitive applications can be supported
                  while maintaining satisfactory quality of service (QoS). Fog nodes can
                  be placed close to IoT source nodes, allowing latency to be noticeably
                  reduced compared to traditional cloud computing. While this example
                  gives an intuitive motivation for fog, latency-sensitive applications are
                  only one of the many applications that warrant the need for fog com-
                  puting. Nodes in fog computing are generally deployed in less central-
                  ized locations compared to centralized cloud data centers. Fog nodes
                  are wide-spread and geographically available in large numbers. In fog
                  computing, security must be provided at the edge or in the dedicated
                  locations of fog nodes, as opposed to the centrally-developed security
                  mechanisms in dedicated buildings for cloud data centers. The decen-
                  tralized nature of fog computing allows devices to either serve as fog
                  computing nodes themselves (e.g. a car acts as a fog node for onboard
                  sensors) or use fog resources as the clients of the fog.
                  The majority of differences between cloud and fog computing are
                  attributed to the scale of hardware components associated with these
                  computing paradigms. Cloud computing provides high availability of
                  computing resources at relatively high power consumption, whereas
                  fog computing provides moderate availability of computing resources
                  at lower power consumption. Cloud computing typically utilizes
                  large data centers, whereas fog computing utilizes small servers, routers,
                  switches, gateways, set-top boxes, or access points. Since hardware for
                  fog computing occupies much less space than that of cloud computing,
                  hardware can be located closer to users. Fog computing can be accessed
                  through connected devices from the edge of the network to the network
                  core, whereas cloud computing must be accessed through the network
                  core. Moreover, continuous Internet connectivity is not essential for the
                  fog-based services to work. That is, the services can work independently
                  with low or no Internet connectivity and send necessary updates to the
                  cloud whenever the connection is available. Cloud computing, on the
                  other hand, requires devices to be connected when the cloud service is
                  in progress.
                  Fog helps devices measure, monitor, process, analyze, and react, and
                  distributes computation, communication, storage, control, and decision
                  making closer to IoT devices. Many industries could
                  use fog to their benefit: energy, manufacturing, transportation, health-
                  care, smart cities, to mention a few.</p>
                  <div class="card2"><a href="https://imgbb.com/"><img src="image/intro2.png" alt="f6" border="0"></a></div>
                  <div class="card2"><a href="https://imgbb.com/"><img src="image/intro3.png" alt="f7" border="0"></a></div>
                  <p class="p1"><b>Fogcloud federation</b></p>
                  
                  <p class="p1">There are clear differences and trade-offs between cloud and fog
                  computing, and one might ask which one to choose. However, fog and
                  cloud complement each other; one cannot replace the need of the other.
                  By coupling cloud and fog computing, the services that connected de-
                  vices use can be optimized even further. Federation between fog and
                  cloud allows enhanced capabilities for data aggregation, processing, and
                  storage. For instance, in a stream processing application, the fog could
                  filter, preprocess, and aggregate traffic streams from source devices,
                  while queries with heavy analytical processing, or archival results could
                  be sent to the cloud. An orchestrator could handle the cooperation be-
                  tween cloud and fog. Specifically, a fog orchestrator could provide an in-
                  teroperable resource pool, deploy and schedule resources to application
                  workflows, and control QoS. Through the use of SDN, fog service
                  providers will have greater control over how the network is configured
                  with a large number of fog nodes that transfer data between the cloud
                  and IoT devices.</p>
                  <p class="p1"><b>Fog RAN</b></p>
                  <p class="p1">
                  Fog computing can be integrated into mobile technologies in the
                  form of radio access networks (RAN), to form what is referred to as fog
                  RAN (F-RAN). Computing resources on F-RANs may be used for caching
                  at the edge of the network, which enables faster retrieval of content and
                  a lower burden on the front-haul. F-RAN can be implemented through
                  5G related mobile technologies. On the other hand, cloud RAN
                  (C-RAN) provides centralized control over F-RAN nodes. C-RAN takes
                  advantage of virtualization, and decouples the base stations within a
                  cell of the mobile network from its baseband functions by virtualizing
                  those functions. In C-RAN a large number of low-cost Remote Ra-
                  dio Heads (RRHs) are randomly deployed and connected to the Base
                  Band Unit (BBU) pool through the front-haul links. Both F-RAN and
                  C-RAN are suited for mobile networks with base stations and are can-
                  didates for 5G deployments. Also, the use of F-RAN and C-RAN brings
                  a more energy efficient form of network operation. We encourage the
                  motivated reader to refer to reference for more information about
                  F-RAN.</p>
                  </div>
                  <div class="card3">
                    <p class="p1"><b>Edge computing</b></p>
                    
                  <p class="p1">Similar to how MCC extends the capabilities of mobile devices,
                    edge computing also enhances the management, storage, and process-
                    ing power of data generated by connected devices. Unlike MCC, edge
                    computing is located at the edge of the network close to IoT devices;
                    note that the edge is not located on the IoT devices, but as close as one
                    hop to them. It is worth noting that the edge can be more than one hop
                    away from IoT devices in the local IoT network. OpenEdge Computing
                    defines edge computing as computation done at the edge of the network
                    through small data centers that are close to users [41]. The original vi-
                    sion for edge computing is to provide compute and storage resources
                    close to the user in open standards and ubiquitous manner [41]. Edge
                    computing is a crucial computing paradigm in the current landscape
                    of IoT devices; it integrates the IoT devices with the cloud by filtering,
                    preprocessing, and aggregating IoT data intelligently via cloud services
                    deployed close to IoT devices [42].
                    Some issues that edge computing is well equipped to handle are pri-
                    vacy, latency, and connectivity. Due to its proximity to the users, latency
                    in edge computing is typically lower than in MCC and cloud computing,
                    if enough local computation power is provided; latency in edge comput-
                    ing can be slower than cloud or MCC if the local computation unit is not
                    powerful enough. Service availability is also higher in edge computing
                    because connected devices do not have to wait for a highly centralized
                    platform to provide a service, nor are connected devices limited by the
                    limited resources in traditional mobile computing. Compared to MACC,
                    edge computing has small data centers, whereas MACC fundamentally
                    does not need data centers. As a result, edge computing has higher
                    service availability. Edge computing also can expand with broader</p>
                  <p class="p1">computing capabilities than MACC by forming hybrid architectures with
                    peer-to-peer and cloud computing models.
                    <b> Edge computing vs. fog computing</b>
                    Although fog computing and edge computing both move the compu-
                    tation and storage to the edge of the network and closer to end-nodes,
                    these paradigms are not identical. In fact, the OpenFog Consortium
                    states that edge computing is often erroneously called fog computing;
                    OpenFog Consortium makes the distinction that fog computing is hierar-
                    chical and it provides computing, networking, storage, control, and ac-
                    celeration anywhere from cloud to things; while, edge computing tends
                    to be limited to computing at the edge. Moreover,
                    in a tutorial article about fog and edge, the authors explain that
                    “fog is inclusive of cloud, core, metro, edge, clients, and things,” and
                    “fog seeks to realize a seamless continuum of computing services from
                    the cloud to the things rather than treating the network edges as isolated
                    computing platforms,” and “fog envisions a horizontal platform that will
                    support the common fog computing functions for multiple industries
                    and application domains, including but not limited to traditional telco
                    services.” 
                    <b>Where is edge?</b>
                     
                    It is worth mentioning that edge computing, cloudlets, fog comput-
                    ing, and mist computing (to be discussed in Section 3.9) are used in-
                    terchangeably in some papers, as they all have “edge” as a common
                    term. The term edge used by the telecommunications industry usu-
                    ally refers to 4G/5G base stations, RANs, and ISP (Internet Service
                    Provider) access/edge networks. Yet, the term edge that is recently
                    used in the IoT landscape refers to the local network where
                    sensors and IoT devices are located. In other words, the edge is the
                    immediate first hop from the IoT devices (not the IoT nodes them-
                    selves), such as the WiFi access points or gateways. If the computation
                    is done on IoT devices themselves, this computing paradigm is referred
                    to as mist computing. General Electric notes that fog
                    computing focuses on interactions between edge devices (e.g., RANs,
                    base stations, or edge routers), whereas edge computing focuses on the
                    technology attached to the connected things (e.g., WiFi access points)
                    </p></div>
                    <div class="card2"><a href="https://ibb.co/MV9Nbv0"><img src="image/intro4.png" alt="f9" border="0"></a></div>
                    <div class="card2"><a href="https://ibb.co/pLdxk68"><img src="image/intro5.png" alt="f8" border="0"></a></div>
                    <button class="btn btn-secondary button " style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);"  onclick="display('sectionid03')">Previous</button>
                    <button class="btn btn-danger button" style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);" onclick="display('sectionarchi')">next</button>
</div>
<div id="sectionarchi" class="modal">
                  <div  style="margin: 20px; padding:15px;"></div>
                <h3 class="heading"  style="color:rgb(255, 255, 255); background-color:black;height:8vh;width:auto;">Architecture</h3>
                <div class="card2">
                <img src="image/archi1.png" alt="Screenshot-2023-08-29-205041" border="10"/>
                </div>
                <div class="card1">
                
                <p class="p1"><b>Data flow architectures: </b>
These architectures are based on the direction of movement of workloads and data in the computing ecosystem. For example, workloads could be transferred from the user devices to the edge nodes or alternatively from cloud servers to the edge nodes
</p>
<p class="p1"><b> Control architectures:</b>
   These architectures are based on how the resources are controlled
  in the computing ecosystem. For example, a single controller or central algorithm may be
  used for managing a number of edge nodes. Alternatively, a distributed approach may be
  employed.
  
</p>
  <p class="p1"><b> Tenancy architecture:</b>
    These architectures are based on the support provided for hosting
    multiple entities in the ecosystem. For example, either a single application or multiple ap-
    plications could be hosted on an edge node.</p>
    </div>
    <div class="card2"><a href="https://ibb.co/Qm0CFcM"><img src="image/archi2.png" alt="f1" border="0"></a></div>
  
    <div class="card3">
    <h3 style="color: rgb(255, 0, 0);">Data Flow</h3>
    <p class="p1">
      
      
      This survey identifies key data flow architectures based on how data or workloads are transferred
      within a fog/edge computing environment. This section considers three data flow architectures,
      namely, aggregation, sharing, and offloading.
      In the aggregation model, an edge node obtains data generated from multi-
      ple end devices that is then partially computed for pruning or filtering. The aim in the aggregation
      model is to reduce communication overheads, including preventing unnecessary traffic from be-
      ing transmitted beyond the edge of the network. Research on aggregation can broadly be classified
      on the basis of (i) techniques for modeling and implementing aggregation and (ii) techniques for
      improving aggregation.
     <b> i. Techniques for Modeling and Implementing Aggregation:</b> The underlying techniques imple-
      mented for supporting aggregation have formed an important part of Wireless Sensor Networks
      (WSNs)  and distributed data stream processing . Dense and large-scale sensor networks
      cannot route all data generated from sensors to a centralized server, but instead need to make use
      of intermediate nodes along the data path that aggregate data. This is referred to as in-network
      data aggregation . We consider WSNs to be predecessors of modern edge computing systems.
      Existing research in the area of in-network data aggregation can be classified into the following six
      ways on the basis of the underlying techniques used for modeling and implementing aggregation:</p>
      <p class="p1"><b> a. Graph-based Techniques:</b></p>
      <p class="p1">
        
         In this survey, we report two graph-based techniques that are used
        for data aggregation, namely, tree-based and directed graph-based techniques.
        Tree-based Techniques: Two examples of tree-based techniques are Data Aggregation Trees
        (DATs) and spatial index trees. DATs are commonly used for aggregation in WSNs using De-
        terministic Network Models (DNMs) or Probabilistic Network Models (PNMs). Recent research
        highlights the use of PNMs over DNMs for making realistic assumptions of lossy links in the net-
        work by using tree-based techniques for achieving load balancing. Spatial index trees are
        employed for querying within networks, but have recently been reported for aggregation. EGF is
        an energy-efficient index tree used for both data collection and aggregation. This technique
        is demonstrated to work well when the sensors are unevenly distributed. The sensors are divided
        into grids, and an index tree is first constructed. Based on the hierarchy, an EGF tree is constructed
        by merging neighboring grids. Multi-region queries are aggregated in-network and then executed.
        Directed Graph-based Techniques: The Dataflow programming model uses a directed graph and is
        used for WSN applications. Recently, a Distributed Dataflow (DDF) programming model has been
        proposed in the context of fog computing. The model is based on the MQTT protocol, supports
        the deployment of flow on multiple nodes, and assumes the heterogeneity of devices.</p>
      
        <p class="p1"><b>  b. Cluster-based Techniques:</b></p>
        <p class="p1">These techniques rely on clustering the nodes in the network. For
        example, energy efficiency could be a key criterion for clustering the nodes. One node from each
        cluster is then chosen to be a cluster head. The cluster head is responsible for local aggregation in
        each cluster and for transmitting the aggregated data to another node. Clustering techniques for
        energy-efficient data aggregation have been reported. It has been highlighted that the spatial
        correlation models of sensor nodes cannot be used accurately in complex networks. Therefore,
        Data Density Correlation Degree (DDCD) clustering has been proposed.</p>
        <p class="p1"><b>  c. Petri Net-based Techniques: </b></p>
       <p class="p1">In contrast to tree-based techniques, recent research highlights
        the use of High Level Petri Net (HLPN), referred to as RedEdge, for modeling aggregation in edge-
        based systems. Given that fog/edge computing accounts for three layers, namely, the cloud,
        the user device, and the edge layers, techniques that support heterogeneity are required. HLPN
        facilitates heterogeneity, and the model is validated by verifying satisfiability using an automated
        solver. The data aggregation strategy was explored for a smart city application and tested for a
        variety of efficiency metrics, such as latency, power, and memory consumption.</p>
        <p class="p1"><b>d. Decoupled Techniques:</b></p>
        <p class="p1">d. Decoupled Techniques: The classic aggregation techniques described above usually exhibit high
        inaccuracies when data is lost in the network. The path for routing data is determined on the basis
        of the aggregation technique. However, Synopsis Diffusion (SD) is a technique proposed for decou-
        pling routing from aggregation so they can be individually optimized to improve accuracy.
        The challenge in SD is that if one of the aggregating nodes is compromised, false aggregations
        will occur. More recently, there has been research to filter outputs from compromised nodes.
        In more recent edge-based systems, Software-Defined Networking (SDN) is employed to decouple
        computing from routing.</p>
        
        <p class="p1"><b> e. Batch Techniques:</b></p>
       <p class="p1"> e. Batch Techniques: This model of aggregation is employed in data stream processing. The data
        generated from a variety of sources is transmitted to a node where the data is grouped at time
        intervals to a batch job. Each batch job then gets executed on the node. For example, the under-
        lying techniques of Apache Flink rely on batch processing of incoming data.1 Similarly, Apache
        Spark 2 employs the concept of Discretized Streams (or D-Streams), a micro-batch processing
        technique that periodically performs batch computations over short time intervals.</p>
        <p class="p1"><b> f. Hybrid Techniques:</b></p>
        <p class="p1">These techniques combine one or more of the techniques considered above.
For example, the Tributary-Delta approach combines tree-based and Synopsis Diffusion (SD) tech-
niques in different regions of the network. The aim is to provide low loss rate and present
few communication errors while maintaining or improving the overall efficiency of the network.</p>

      </div>
      <div class="card1">
        <p class="p1">
       <b>i. Techniques for Improving Aggregation:</b>  Aggregation can be implemented, such that it optimizes
different objectives in the computing environment. These objectives range from communication
efficiency in terms of bandwidth, latency, and energy constraints (that are popularly used) to the
actual quality of aggregation (or analytics) that is performed on the edge node. The following is a
classification obtained after surveying existing research on techniques for improving aggregation:</p>
<p class="p1"><b>a. Efficiency-aware Techniques:</b></p>
<p class="p1">
 We present three categories of efficiency-aware techniques: the
first for optimizing bandwidth, the second for minimizing latency, and the third for reducing en-
ergy consumption.
Bandwidth-aware: The Bandwidth Efficient Cluster-based Data Aggregation (BECDA) algorithm
has three phases. First, distributed nodes are organized into a number of clusters. Then, each
cluster elects a cluster head that aggregates data from within the cluster. Thereafter, each cluster
head contributes to intra-cluster aggregation. This approach utilizes bandwidth efficiently for data
aggregation in a network and is more efficient than predecessor methods.
Latency-aware: Another important metric that is often considered in edge-based systems for
aggregation includes latency. A mediation architecture has been proposed in the context
of data services for reducing latency. In this architecture, policies for filtering data produced
by the source based on concepts of complex event processing are proposed. In the experimental
model, requests are serviced in near real-time with minimum latency. There is a trade-off against
energy efficiency when attempting to minimize latency. Therefore, techniques to keep latency
to a minimum while maintaining constant energy consumption were employed.
Energy-aware: Research in energy efficiency of data aggregation focuses on reducing the power
consumption of the network by making individual nodes efficient via hardware and software tech-
niques. For example, in a multi-hop WSN, the energy consumption trade-off with aggregation
latency has been explored under the physical interference model. A successive interference
cancellation technique was used, and an energy-efficient minimum latency data aggregation al-
gorithm proposed. The algorithm achieves lower bounds of latency while maintaining constant
energy. In a mobile device based edge computing framework, RedEdge, it was observed that the
energy consumption for data transfer was minimized. However, there is a data processing
overhead on the edge node. Energy awareness techniques for edge nodes are an open research
area.</p>
<p class="p1"><b>b. Quality-aware Techniques: </b></p>
<p class="p1"> Selective forwarding is a technique in which data from end devices
are conditionally transmitted to a node for reducing overheads. “Quality-aware” in this context
refers to making dynamic decisions for improving the quality of predictive analytics in selective
forwarding. In a recent study, the optimal stopping theory was used for maximizing the qual-
ity of aggregation without compromising the efficiency of communication. It was noted that
instantaneous decision-making that is typically employed in selective forwarding does not ac-
count for the historical accuracy of prediction. Quality awareness is brought into this method by
proposing optimal vector forwarding models that account for historical quality of prediction.</p>
<p class="p1"><b>c. Security-aware Techniques:  </b></p>
<p class="p1">
Aggregation occurring at an edge node between user devices and
a public cloud needs to be secure and ensure identity privacy. An Anonymous and Secure Aggre-
gation (ASAS) scheme in a fog environment using elliptic curve public-key cryptography,
bilinear pairings, and a linearly holomorphic cryptosystem, namely, the Castagnos-Laguillaumie
cryptosystem, has been developed. Another recently proposed technique includes the Light-
weight Privacy-preserving Data Aggregation (LPDA) for fog computing. LPDA, contrary to
ASAS, is underpinned by the homomorphic Paillier encryption, the Chinese Remainder Theorem,
and one-way hash chain techniques. Other examples of privacy-aware techniques include those
employed in fog computing-based vehicle-to-infrastructure data aggregation.
</p>
<div class="card2"><a href="https://ibb.co/nb6CJVp"><img src="image/archi3.png" alt="f2" border="0"></a></div>
<p class="p1"><b>d. Heterogeneity-aware Techniques:</b></p>
<p class="p1">
 Edge-based environments are inherently heterogeneous. Heterogeneity of resources here is a reference to different types of fog/edge nodes,
including CPU architectures, combination of dedicated micro data centers (CPU-based systems),
and traffic routing devices such as routers, base stations, and switches at the edge of the network.
Traditional cloud techniques for data aggregation have assumed homogeneous hardware, but there
is a need to account for heterogeneity. Some research takes heterogeneous nodes into account for
data aggregation in WSNs. Heterogeneous edge computing is still in its infancy.
While there is indication of the possibility to use heterogeneous resources in an ideal fog/edge
computing model, there is little evidence of such a system that is fully implemented.
</p> 


<p class="p1"><b> 2.1.2 Sharing.</b></p>
 
<p class= "p1">
 Contrary to the aggregation model, the sharing model is usually employed when
the workload is shared among peers. This model aims at satisfying computing requirements of a
workload on a mobile device without offloading it into the cloud, but onto peer devices that are
likely to be battery-powered. This results in a more dynamic network given that devices may join
and leave the network without notice. Practically feasible techniques proposed for cooperative task
execution will need to be inherently energy-aware. Research in this area is generally pursued under
the umbrella of Mobile Cloud Computing (MCC)  and Mobile Edge Computing (MEC) 
and is a successor to peer-to-peer computing .
Research on techniques for sharing can be classified into the following three ways</p>

<p class="p1"><b>i. Based on Control:</b></p>

<p class="p1"></p>Research on control of the sharing model employed in mobile edge devices
can be distinguished on the basis of 
(a) centralized control and 
(b) distributed control.</p>
<p class="p1"><b>a. Centralized Control:</b></p>
 
<p class="p1">In this technique, a centralized controller is used to manage the work-
load on each edge device in a network. For example, a collection of devices at the edge is modeled
as a Directed Acyclic Graph (DAG)-based workflow. The coordination of executing tasks resides
with a controller in the cloud . A Software Defined Cooperative Offloading Model (SD-
COM) was implemented based on Software Defined Networking (SDN). A controller is placed
on a Packet Delivery Network (PDN) gateway that is used to enable cooperation between mobile
devices connected to the controller. The controller aims at reducing traffic on the gateway and
ensuring fairness in energy consumption between mobile devices. To deal with dynamically ar-
riving tasks, an Online Task Scheduling (OTS) algorithm was developed.

Centralized techniques are fairly common in the literature, since they are easier to implement.
However, they suffer from scalability and single point failures as is common in most centralized
systems.</p>
<p class="p1"><b>b. Distributed Control:</b></p>
 
<p class="p1">In the area of distributed control among edge devices, there seems to be
relatively limited research. A game theoretic approach was employed as a decentralized approach
for achieving the Nash equilibrium among cooperative devices. The concept of the Nash equi-
librium in the sharing model is taken further to develop the Multi-item Auction (MIA) model and
Congestion Game (COG)-based sharing.</p>
<p class="p1"><b>ii. Based on Adaptive Techniques:</b></p>

<p class="p1">These techniques are nature-inspired and solve multi-objective
optimization problems . There are different objectives in a system that employs a sharing
model. For example, the sharing model at the edge can be employed in a battlefield scenario.
In this context, latencies need to be minimum, and the energy consumption of the devices needs
to be at optimum. Based on existing research, the following adaptive techniques are considered:</p>
<p class="p1"><b>a. Connectivity-aware:</b></p>

<p class="p1">The sharing model needs to know the connectivity between devices, for
example, in the above battlefield scenario. A mobile device augments its computing when peer
devices come within its communication range. Then a probabilistic model predicts whether
a task potentially scheduled on a peer device can complete execution in time when it is in the
coverage of the device. Connectivity-aware techniques can be single hop, multi-hop, or oppor-
tunistic.
Single Hop Techniques: In this technique, a device receives a list of its neighbors that form a fully
connected network. When a workload is shared by a device, the workload will be distributed to
other devices that are directly connected to the device.
Multi-hop Techniques: Each device computes the shortest path to every other node in the net-
work that can share its workload. The work is usually shared with devices that may reduce the
overall energy footprint. The benefit of a multi-hop technique in the sharing model compared to
single-hop techniques is that a larger pool of resources can be tapped into for more computa-
tionally intensive workloads. A task distribution approach using a greedy algorithm to reduce the
overall execution time of a distributed workload was recently proposed.
Opportunistic Techniques: The device that needs to share its workload in these techniques checks
whether its peers can execute a task when it is within the communication range. This is predicted
via contextual profiling or historical data of how long a device was within the communication
range of its peers. In recent research, a connectivity-aware opportunistic approach was designed
such that:
 (i) data and code for the job can be delivered in a timely manner, 
 (ii) sequential jobs are
executed on the same device so intermediate data does not have to be sent across the network,
and 
(iii) there is distributed control, and jobs are loosely coupled.
The jobs are represented as a Directed Acyclic Graph (DAG), and the smallest component of a job is called a PNP-block,
which is used as the unit scheduled onto a device. In the context of Internet-of-Things (IoT) for
data-centric services, it is proposed that a collection of mobile devices forms a mobile cloud via
opportunistic networking to service the requests of multiple IoT sensors.</p>

<p class="p1"><b>b. Heterogeneity-aware: </b></p>

<p class="p1">Edge devices in a mobile cloud are heterogeneous at all levels. Therefore,
the processor architecture, operating system, and workload deployment pose several challenges
in facilitating cooperation. There is research that assumes that the architectures of the co-
operating edge are similar, but have different energy and memory or system utilization require-
ments. These parameters are used for coded computation. There is recent research tackling
heterogeneity-related issues in mobile networks. For example, a work-sharing approach named
Honeybee was proposed in which cycles of heterogeneous mobile devices are used to serve the
workload from a given device. The approach accounts for devices leaving/joining the system.
Similarly, a framework based on service-oriented utility functions was proposed for managing
heterogeneous resources that share tasks. A resource coordinator delegates tasks to resources
in the network so parameters, such as gain and energy, are optimized using convex optimization
techniques.</p>
<p class="p1"><b>c. Security-aware:</b></p>

<p class="p1">A technique to identify and isolate malicious attacks that could exist in a
device used in the sharing model, referred to as HoneyBot, has been proposed. A few of the
devices in a mobile network are chosen as HoneyBots for monitoring malicious behavior. In the
provided experimental results, a malicious device can be identified in 20 minutes. Once a device is
identified to be malicious, it is isolated from the network to keep the network safe.
d. Fairness-aware:
Fairness has been defined as a multi-objective optimization problem. The ob-
jectives are to reduce the drain on the battery of mobile devices to prolong the network lifetime,
and at the same time improve the performance gain of the workload shared between devices.
The processing chain of mobile applications was modeled as a DAG and assumed that each node
of the DAG is an embarrassingly parallel task. Each task was considered as a Multi-objective Com-
binatorial Bottleneck Problem (M-CBP) solved using a heuristic technique.</p>
<p class="p1">iii. Based on Cooperation: Edge devices can share workloads 
(a) either in a less defined environment that is based on ad hoc cooperation or (b) in a more tightly coupled environment where there
is infrastructure to facilitate cooperation.</p>
<p class="p1"><b>a. Ad Hoc Cooperation:</b></p>
<p class="p1">Setting up ad hoc networks for device-to-device communication is not a
new area of research. Ad hoc cooperation has been reported for MCC in the context of the sharing
model for the edge. There is recent research that has coined the term “transient clouds,” in
which neighboring mobile devices form an ad hoc cloud and the underlying task management
algorithm is based on a variant of the Hungarian method.</p>
<p class="p1"><b>b. Infrastructure-based Cooperation:</b></p>
<p class="p1">There is research on the federation of devices at the edge of
the network to facilitate cooperation. This results in more tightly coupled coalitions than ad
hoc clouds, and more cost effectiveness than dedicated micro cloud deployment.Offloading is a technique in which a server, an application, and the associated
data are moved onto the edge of the network. This either augments the computing requirements
of individual or a collection of user devices, or brings services in the cloud that process requests
from devices closer to the source. Research in offloading can be differentiated in the following two
ways, as presented in Figure 7:
i. Offloading from User Device to Edge: This technique augments computing in user devices by
making use of edge nodes (usually a single hop away). The two main techniques used are applica-
tion partitioning and caching mechanisms.</p>

<p class="p1"><b>a. Application Partitioning:</b></p>
<p class="p1">One example of offloading from devices to the edge via application
partitioning is in the GigaSight architecture in which Cloudlet VMs are used to process videos
streamed from multiple mobile devices. The Cloudlet VM is used for denaturing, a process
of removing user-specific content for preserving privacy. The architecture employed is presented
as a Content Delivering Network (CDN) in reverse. In this survey, we discuss the following four
approaches and three models used for application partitioning.
Approaches: Four approaches are considered, namely, brute force, greedy heuristic, simulated
annealing, and fuzzy logic.
Brute Force: There is a study under the umbrella of ENGINE that proposes an exhaustive brute-
force approach, in which all possible combinations of offloading plans (taking the cloud, edge
nodes, and user devices) are explored. The plan with the minimum execution time for a task
is then chosen. This approach simply is not a practical solution given the time needed to derive a
plan, but instead could provide insight into the search space.
Greedy Heuristic: ENGINE also incorporates a greedy approach that focuses on merely minimiz-
ing the time taken for completing the execution of a task on the mobile device. An offloading

Fig. 7. A classification of offloading techniques.
plan is initially generated for each task on a mobile device, and then iteratively refined to keep
the total monetary costs low. Similarly, FogTorch, a prototype implementation of an offloading
framework, uses a greedy heuristic approach for deriving offloading plans.</p>
<p class="p1"><b>Simulated Annealing:</b></p>

<p class="p1">Another approach is simulated annealing, in which the search space is
based on the utilization of fog and cloud nodes, total costs, and the completion time of an applica-
tion to obtain an offloading plan that minimizes the costs and the completion time of the task.</p>
<div class="card2"><a href="https://ibb.co/2qqXSvg"><img src="image/archi4.png" alt="f3" border="0"></a></div>
<p class="p1"><b>Fuzzy Logic:</b></p>


<p class="p1">There is research highlighting that an application from a user device can be par-
titioned and placed on fog nodes using fuzzy logic . The goal is to improve the Quality-
of-Experience (QoE) measured by multiple parameters such as service access rate, resource re-
quirements, and sensitivity toward data processing delay. Fuzzy logic is used to prioritize each
application placement request by considering the computational capabilities of the node.</p>
<p class="p1"><b>Models: </b></p>

<p class="p1">The three underlying models used for application partitioning from devices to the edge
are graph-based, component-based, and neural network-based.</p>
<p class="p1"><b>Graph-based:</b></p>

<p class="p1">CloneCloud employs a graph-based model for the automated partitioning of an
application. Applications running on a mobile device are partitioned and then offloaded onto
device clones in the cloud. In the runtime, this concept translates to migrating the application
thread onto the clone, after which it is brought back onto the original mobile device. Similarly, in
another graph-based approach, each mobile application task to be partitioned is represented as a
Directed Acyclic Graph (DAG). The model assumes that the execution time, migration time,
and data that need to be migrated for each task are known a priori via profiling. Aspect-oriented
programming is then used to obtain traces of sample benchmarks. Thereafter, a trace simulation
is used to determine whether offloading to the edge nodes would reduce execution time.
Component-based: In this case, the functionalities of an application (a web browser) that runs
on a device are modeled as components that are partitioned between the edge server and the
device. The example demonstrated is Edge Accelerated Web Browsing (EAB), in which indi-
vidual components of a browser are partitioned across the edge and the device. The contents of a
web page are fetched and evaluated on the edge while the EAB client merely displays the output.</p>
<p class="p1"><b>Neural Networkbased:</b></p>

<p class="p1">Recent research highlights the distribution of deep neural networks
across user devices, edge nodes, and the cloud . The obvious benefit is that the latency of
inferring from a deep neural network is reduced for latency-critical applications without the need
to transmit images/video far from the source. Deep networks typically have multiple layers that
can be distributed over different nodes. The Neurosurgeon framework models the partitioning
between layers that will be latency- and energy-efficient from end-to-end. The framework
predicts the energy consumption at different points of partitioning in the network and chooses a
partition that minimizes data transfer and consumes the least energy. This research was extended
towards distributing neural networks across geographically distributed edge nodes.</p>
<p class="p1"><b>b. Caching Mechanisms:</b></p>
 
<p class="p1">This is an alternative to application offloading. In this mechanism, a
global cache is made available on an edge node that acts as a shared memory for multiple de-
vices that need to interact. This survey identifies two such mechanisms, namely, chunking and
aggregation, and a reverse auction gamebased mechanism.</p>

<p class="p1"><b>Chunking and Aggregation:</b></p>
<p class="p1">The multi Radio Access Technology (multi-RAT) was proposed as
an architecture for upload caching. In this model, VMs are located at the edge of the network, and
a user device uploads chunks of a large file onto them in parallel. Thereafter, an Aggregation
VM combines these chunks that are then moved onto a cloud server.
Reverse Auction Gamebased: An alternate caching mechanism based on cooperation of edge
nodes was proposed in Reference. The users generate videos that are shared between the
users via edge caching. The mechanism uses a reverse auction game to incentivize caching.
ii. Offloading from the Cloud to the Edge: The direction of data flow is opposite of that considered
above; in this case, a workload is moved from the cloud to the edge. There are three techniques
that are identified in this survey, including server offloading, caching mechanisms, and web programming.</p>
<p class="p1"><b>a. Server Offloading:</b></p>
<p class="p1">A server that executes on the cloud is offloaded to the edge via either repli-
cation or partitioning. The former is a naive approach that assumes that a server on the cloud can
be replicated on the edge.
Replication: Database cloning and application data replication are considered.
Database Cloning: The database of an application may be replicated at the edge of the network
and can be shared by different applications or users.
Application-specific Data Replication: In contrast to database cloning, a specific application may
choose to bring data relevant to the users to the edge for the seamless execution of the applica-
tion. However, both database cloning and application-specific data replication assume that
edge nodes are not storage-limited, so they may not be feasible in resource-constrained edge en-
vironments.
Partitioning:
 We now consider the server-partitioning parameters that are taken into account in
offloading from the cloud to the edge. The parameters considered in partitioning are functionality-
aware, geography-aware, and latency-aware.
Functionality-aware: Cognitive assistance applications, for example Google Glass, are latency-
critical applications, and the processing required for these applications cannot be provided by the
cloud alone. Therefore, there is research on offloading the required computation onto Cloudlet
VMs to meet the processing and latency demands of cognitive assistance applications. The
Gabriel platform built on OpenStack++ is employed for VM management via a control VM, and
for deploying image/face recognition functionalities using a cognitive VM on Cloudlet.
Geography-aware: The service requests of online games, such as PokeMon Go, are typically
transmitted from user devices to a cloud server. Instead of sending traffic to data centers, the
ENORM framework partitions the game server and deploys it on an edge node. Geographical
data relevant to a specific location is then made available on an edge node. Users from the relevant
geographical region connect to the edge node and are serviced as if they were connected to the
data center. ENORM proposes an auto-scaling mechanism to manage the workload for maximizing
the performance of containers that are hosted on the edge by periodically monitoring resource
utilization.
Latency-aware: Similar to ENORM, a study by Báguena et al. aimed at partitioning the back-
end of an application logic traditionally located on clouds to service application requests in real-
time. In the proposed hybrid edge-assisted execution model for LTE networks, application
requests are serviced by both the cloud and the edge networks based on latency requirements.
This differs from the ENORM framework, in which the server is partitioned along geographical
requirements.</p>
<p class="p1"><b>b. Caching Mechanisms:</b></p>

<p class="p1">Content popularity and multi-layer caching are identified.
Content Popularitybased: Content-Delivery Networks (CDNs) and ISP-based caching are tech-
niques employed to alleviate congestion in the network when downloading apps on user devices.
However, there are significant challenges arising from the growing number of devices and apps.
A study by Bhardwaj et al. presented the concept of caching mechanisms specific to apps on edge
nodes, such as routers and small cells, referred to as eBoxes . This concept is called AppSa-
chets and employs two caching strategies: based on popularity and based on the cost of caching.
The research was validated on Internet traffic originating from all users at the Georgia Institute of
Technology for a period of three months.
Similarly, there is research aimed at caching data at base stations that will be employed in 5G
networks . To achieve this, traffic is monitored to estimate content popularity using a Hadoop
cluster. Based on the estimate, content is proactively cached at a base station.
Multi-layer Caching: Multi-layer caching is a technique used in content delivery for Wireless
Sensor Networks (WSNs) . The model assumes that a global cache is available at a base station
that can cache data from data centers, and that localized caches are available on edge nodes. Two
strategies are employed in this technique. The first is uncoded caching, in which each node is
oblivious of the cache content of other nodes, and therefore no coordination of data is required.
The second technique is coded caching, in which the cached content is coded such that all edge
nodes are required to encode the content for the users.
Other miscellaneous techniques are used to support offloading from the cloud to the edge. These
are application-specific, and are determined by the way the application is programmed. For exam-
ple, there is research highlighting the use of web programming that makes use of the client-edge-
server architecture, such that some component of the client executes in edge nodes. The Spaceify
ecosystem enables the execution of Spacelets on edge nodes that are embedded JavaScripts that
use the edge nodes to execute tasks to service user requests . An indoor navigation use-case
is demonstrated for validating the Spaceify concept.</p>
<div class="card2"><a href="https://imgbb.com/"><img src="image/archi5.png" alt="f4" border="0"></a></div>
<p class="p1"><b>2.2 Control</b></p>

<p class="p1">A second method for classifying architectures for resource management in fog/edge environments
is based on control of the resources. This survey identifies two such architectures, namely, cen-
tralized and distributed control architectures Centralized control refers to
the use of a single controller that makes decisions on the computations, networks, or communica-
tion of the edge resources. On the contrary, when decision-making is distributed across the edge
nodes, we refer to the architecture as distributed. This section extends the discussion on control
techniques that was previously presented on sharing techniques in the survey.</p>
<p class="p1"><b>2.2.1 Centralized</b></p>
<p class="p1">There is a lot of research on centralized architectures, but we identify two
centralized architectures, namely, (i) solver-based and (ii) graph matchingbased.

 A classification of control architectures for resource management in fog/edge computing.
i. Solver-based: Mathematical solvers are commonly used for generating deployment and rede-
ployment plans for scheduling workloads in grids, clusters, and clouds. Similar approaches have
been adopted for edge environments. For example, a Least Processing Cost First (LPCF) method
was proposed for managing task allocation in edge nodes . The method is underpinned by a
solver aimed at minimizing processing costs and optimizing network costs. The solver is executed
on a centralized controller for generating the assignment plan.
ii. Graph Matchingbased: An offloading framework that accounts for device-to-device and cloud
offloading techniques was proposed. Tasks were offloaded via a three-layer graph-matching
algorithm that is first constructed by taking the offloading space (mobiles, edge nodes, and the
cloud) into account. The problem of minimizing the execution time of the entire task is mapped
onto the minimum weight-matching problem in the three-layer graph. A centralized technique
using the Blossom algorithm was used to generate a plan for offloading.</p>

<p class="p1"><b>2.2.2 Distributed.</b></p>
<p class="p1">Three distributed architectures are identified: (i) blockchain-based, (ii) game
theoreticbased, and (iii) genetic algorithmbased.</p>
<p class="p1"><b>i. Blockchain-based:</b></p>
<p class="p1">Blockchain technology is used as an underpinning technique for implement-
ing distributed control in edge computing systems . The technique is built on the IEC 61499
standard that is a generic standard for distributed control systems. In this model, Function Blocks,
an abstraction of the process, was used as an atomic unit of execution. Blockchains make it pos-
sible to create a distributed peer-to-peer network without having intermediaries, and therefore
naturally lend themselves to the edge computing model in which nodes at the edge of the network
can communicate without mediators. The Hyperledger Fabric, a distributed ledger platform used
for running and enforcing user-defined smart contracts securely, was used.</p>
<p class="p1"><b>ii. Game Theoretic Approachbased:</b></p>
<p class="p1">The game-theoretic approach is used for achieving dis-
tributed control for offloading tasks in the multi-channel wireless interference environment of
mobile-edge cloud computing . It was demonstrated that finding an optimal solution via cen-
tralized methods is NP-hard. Therefore, the game-theoretic approach is very suitable in such en-
vironments. The Nash equilibrium was achieved for distributed offloading, while two metrics,
namely, the number of benefiting cloud users and the system-wide computational overhead, were
explored to validate the feasibility of the game-theoretic approach over centralized methods.</p>
<p class="p1"><b>iii. Genetic Algorithmbased:</b></p>
<p class="p1">Typically, in IoT-based systems, the end devices are sensors that
send data over a network to a computing node that makes all the decisions regarding all aspects
of networking, communication, and computation. The Edge Mesh approach aims at distributing
decision-making across different edge nodes . For this purpose, Edge Mesh uses a computation
overlay network along with a genetic algorithm to map a task graph onto the communication
network to minimize energy consumption. The variables considered in the genetic algorithm are
the Generation Gap used for crossover operations, mutation rate, and population size.
Additionally, there are other upcoming concepts, such as sensor function virtualization (SFV),
which can support distributed decision-making. SFV modularizes and deploys sensor functions

 A taxonomy of tenancy-based architectures for resource management in Fog/Edge computing.
anywhere in an IoT network . The advantage of the SFV technique is that modules can be
added at runtime on multiple nodes. SFV as a concept is still in its infancy and needs to be demon-
strated in a real-world IoT testbed.</p>
<div class="card2"><a href="https://imgbb.com/"><img src="image/archi6.png" alt="f5" border="0"></a></div>
<p class="p1"><b>2.3 Tenancy</b></p>

<p class="p1">A third method for classifying architectures for resource management in fog/edge environments
is tenancy. The term tenancy in distributed systems refers to whether or not underlying hardware
resources are shared between multiple entities for optimizing resource utilization and energy effi-
ciency. A single-tenant system refers to the exclusive use of the hardware by an entity. Conversely,
a multi-tenant system refers to multiple entities sharing the same resource. An ideal distributed
system that is publicly accessible needs to be multi-tenant.
The OpenFog reference architecture highlights multi-tenancy as an essential feature in fog/edge
computing . An application server may be offloaded from the cloud to the edge and service
users. Therefore, the entities that share the hardware resources in this context are the applications
that are hosted on the edge, and the users that are serviced by the edge server.
In this article, we propose a classification of tenancy in fog/edge computing in two dimensions—
applications and users. As shown in Figure 9, the following are the four possibilities in the taxonomy:</p>

<p class="p1"><b>i. Single Application, Single User (SASU):</b></p>
<p class="p1">The edge node executes one application, and only
the user can connect to the application. The application and the user solely use the hard-
ware resources. The infrastructure is likely to be a private experimental test-bed.</p>
<p class="p1"><b>ii. Single Application, Multiple User (SAMU):</b></p>
<p class="p1">The edge node executes one application that sup-
ports multiple users. Although the underlying hardware resources are not shared among
applications, there is a higher degree of sharing than SASU, since multiple user requests
are serviced by the edge node.</p>
<p class="p1"><b>iii. Multiple Application, Single User (MASU):</b></p>
<p class="p1">The edge node hosts multiple applications, but
each application can only support a single user. This form of tenancy may be used for
experimental purposes (or stress-testing the system) during the development of an ideal
infrastructure.</p>
<p class="p1"><b>iv. Multiple Application, Multiple User (MAMU):</b></p>
<p class="p1">The edge node hosts multiple applications,
and many users can connect to an individual application. This is an ideal infrastructure
and is representative of a publicly accessible infrastructure.
There are two techniques that support multi-tenancy, namely, system virtualization and net-
work slicing.
(1) System Virtualization: At the system level, virtualization is a technique employed to sup-
port multi-tenancy. A variety of virtualization technologies are currently available, such as
traditional virtual machines (VMs) and containers. VMs have
a larger resource footprint than containers. Therefore, lightweight virtualization currently
utilized in edge computing incorporates the latter. Virtualization makes it
possible to isolate resources for individual applications, whereby users can access appli-
cations hosted in a virtualized environment. For example, different containers of multiple
applications may be concurrently hosted on an edge node.
(2) Network Slicing: At the network level, multiple logical networks can be run on top of the
physical network, so different entities with different latency and throughput requirements
may communicate across the same physical network. The key principles of Software
Defined Networking (SDN) and Network Functions Virtualization (NFV) form the basis
of slicing. The ongoing European project SESAME 4 (Small
cells coordination for Multi-tenancy and Edge services) tackles the challenges posed by
network slicing. The network bandwidth may also be partitioned across tenants, and also
referred to as slicing. EyeQ is a framework that supports fine-grained control of network
bandwidth for edge-based applications. The framework provides end-to-end mini-
mum bandwidth guarantees, thereby providing an efficient implementation for network
performance isolation at the edge.</p>
<button class="btn btn-secondary button " style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);" onclick="display('sectionintro')">Previous</button>
<button class="btn btn-danger button" style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);" onclick="display('sectionalg')">next</button>
      </div>


</div>
<div id="sectionalg" class="model">
  <div class="card1">
    <h3 class="heading"  style="color:rgb(255, 255, 255); background-color:black;height:8vh;width:auto;">Algorithm</h3>
                
  <p class="p1"><b>ALGORITHMS </b></p>
  
  <p class="p1">There are several underlying algorithms used to facilitate fog/edge computing. In this section,
  we discuss four algorithms, namely, (i) discovery—identifying edge resources within the network
  that can be used for distributed computation, (ii) benchmarking—capturing the performance of
  resources for decision-making to maximize the performance of deployments, (iii) load-balancing—
  distributing workloads across resources based on different criteria such as priorities, fairness, and
  so on, and (iv) placement—identifying resources appropriate for deploying a workload. 
  </p>
  <p class="p1"><b>Discovery</b> </p>
  <p class="p1">Discovery refers to identifying edge resources so workloads from the clouds or from user de-
  vices/sensors can be deployed on them. Typically, edge computing research assumes that edge
  resources are discovered. However, this is not an easy task . Three techniques that use pro-
  gramming infrastructure, handshaking protocols, and message passing are employed in discovery.
  The first technique uses programming infrastructure such as Foglets, proposed as a mecha-
  nism for edge resources to join a cloud-edge ecosystem. A discovery protocol was proposed
  that matches the resource requirements of an application against available resources on the edge.
  Nonetheless, the protocol assumes that the edge resource is publicly known or available for use.
  
  An additional join protocol is implemented that allows the selection of one edge node from among
  a set of resources that have the same geographic distance from the user.
  The second technique uses handshaking protocols. The Edge-as-a-Service (EaaS) platform
  presents a lightweight discovery protocol for a collection of homogeneous edge resources .
  The platform requires a master node that may be a compute available network device or a ded-
  icated node that executes a manager process and communicates with edge nodes. The manager
  communicates with potential edge nodes and executes a process on the edge node to run com-
  mands. Once discovered, the Docker or LXD containers can be deployed on edge nodes.
  The benefit of the EaaS platform is that the discovery protocol implemented is lightweight, and
  the overhead is only a few seconds for launching, starting, stopping, or terminating containers.
  Up to 50 containers with an online game workload similar to PokeMon Go were launched on an
  individual edge node. However, this has been carried out in the context of a single collection of
  edge nodes. Further research will be required to employ such a model in a federated edge environ-
  ment. The major drawback of the EaaS platform is that it assumes a centralized master node that
  can communicate with all potential edge nodes. The handshaking protocol assumes that the edge
  nodes can be queried and can be made available in a common marketplace via owners. In addition,
  the security-related implications of the master node installing a manager on the edge node and
  executing commands on it was not considered.
  The third technique for discovery uses message passing. In the context of a sensor network in
  which the end devices may not necessarily have access to the Internet, there is research suggesting
  that messages may be delivered in such a network using services offered by the nodes (referred
  to as processing nodes) connected to the Internet [91]. A discovery method for identifying the
  processing nodes was presented. The research assumed that a user can communicate with any
  node in a network and submit queries, and relies on simulation-based validation.</p>
  <p class="p1"><b>Benchmarking</b> </p>
  
  <p class="p1">Benchmarking is a de facto approach for capturing the performance (of entities such as memory,
  CPU, storage, network, etc.) of a computing system [174]. Metrics relevant to the performance of
  each entity need to be captured using standard performance evaluation tools. Typical tools used
  for clusters or supercomputers include LINPACK [46] or NAS Parallel Benchmarks [10].
  On the cloud, this is performed by running sample micro or macro applications that stress-tests
  each entity to obtain a snapshot of the performance of a Virtual Machine (VM) at any given point
  in time [49, 134]. The key challenge of benchmarking a dynamic computing system (where work-
  loads and conditions change significantly, such as the cloud and the edge) is obtaining metrics
  in near real-time [174, 175]. Existing benchmarking techniques for the cloud are time-consuming
  and are not practical solutions, because they incur a lot of monetary costs. For example, accu-
  rately benchmarking a VM with 200GB RAM and 1TB storage requires a few hours. Alternative
  lightweight benchmarking techniques using containers have been proposed that can obtain results
  more quickly on the cloud than traditional techniques [93, 178]. However, a few minutes are still
  required to get results comparable to traditional benchmarking.
  Edge benchmarking can be classified into: (i) benchmarking for evaluating functional proper-
  ties, (ii) application-based benchmarking, and (iii) integrated benchmarking. The majority of edge
  benchmarking research evaluates power, CPU, and memory performance of edge processors [117].
  Benchmarking becomes more challenging in an edge environment for a number of reasons.
  First, because edge-specific application benchmarks that capture a variety of workloads are not
  yet available. Existing benchmarks are typically scientific applications that are less suited for the
  edge [34]. Instead, voice-driven benchmarks [162] and Internet-of-Things (IoT) applications have
  been used [95]. Benchmarking object stores in edge environments have also been proposed [39].
  
  Second, running additional time-consuming applications on resource-constrained edge nodes
  can be challenging. Spark has been evaluated in a highly resource-constrained fog environment
  consisting of eight Raspberry Pi single-board computers [69]. The job completion time of Spark is
  reduced significantly with the cluster of Raspberry Pi computers, but it still requires a few minutes
  to get results. Instead of running time-consuming applications, CloudSim [24] has been used to
  simulate edge workloads for estimating resource usage and developing a pricing model in fog
  computing [2]. There is a need for lightweight benchmarking tools for the edge.
  Finally, it is not sufficient to merely benchmark edge resources, but an integrated approach for
  benchmarking cloud and edge resources is required [51]. This will ensure that the performance
  of all possible combinations of deployments of the application across the cloud and the edge is
  considered for maximizing overall application performance.</p>
  <p class="p1"><b>Load-balancing</b> </p>
  
  <p class="p1">As edge data centers are deployed across the network edge, the issue of distributing tasks using
  an efficient load-balancing algorithm has gained significant attention. Existing load-balancing al-
  gorithms at the edge employ four techniques, namely, optimization techniques, cooperative load
  balancing, graph-based balancing, and using breadth-first search.
  He et al. [70] proposed the Software Defined Cloud/Fog Networking (SDCFN) architecture for
  the Internet of Vehicles (IoV). SDCFN allows centralized control for networking between vehicles
  and helps the middleware to obtain the required information for load balancing. The study adopted
  Particle Swarm Optimization - Constrained Optimization (PSO-CO) [136] for load-balancing to de-
  crease latency and effectively achieve the required quality of service (QoS) for vehicles.
  CooLoad [15] proposed a cooperative load-balancing model between fog/edge data centers to de-
  crease service suspension time. CooLoad assigns each data center a buffer to receive requests from
  clients. When the number of items in the buffer is above a certain threshold, incoming requests
  to the data center are load-balanced to an adjacent data center. This work assumed that the data
  centers were connected by a high-speed transport for effective load balancing.
  Song et al. [130] pointed out that existing load-balancing algorithms for cloud platforms that
  operate in a single cluster cannot be directly applied to a dynamic and peer-to-peer fog computing
  architecture. To realize efficient load-balancing, they abstracted the fog architecture as a graph
  model where each vertex indicates a node, and the graph edge denotes data dependency between
  tasks. A dynamic graph-repartitioning algorithm that uses previous the load-balancing result as
  input and minimizes the difference between the load-balancing result and the original status was
  proposed.
  Puthal et al. focused on developing an efficient dynamic load-balancing algorithm with an au-
  thentication method for edge data centers [138]. Tasks were assigned to an under-utilized edge data
  center by applying the Breadth First Search (BFS) method. Each data center was modeled using the
  current load and the maximum capacity used to compute the current load. The authentication
  method allows the load-balancing algorithm to find an authenticated data center.</p>
  <p class="p1"><b> Placement</b> </p>
  
  <p class="p1">One challenging issue in fog/edge computing is to place incoming computation tasks on suitable
  fog/edge resources. Placement algorithms address this issue and need to consider the availability
  of resources in the fog/edge layer and the environmental changes . Existing techniques can
  be classified as dynamic condition-aware techniques and iterative techniques. Iterative techniques
  can be further divided into two spaces: iterative over resources, and iterative over the problem
  spaces.
  
  Wang et al. pointed out that existing work solved placement issues in fog/edge computing under
  static network conditions and predetermined resource demands and were not dynamic condition-
  aware. This short-
  coming was addressed by considering an additional set of parameters including the location and
  preference of a user, database location, and the load on the system. A method that predicts the val-
  ues of the parameters when service instances of a user are placed in a certain configuration was
  proposed. The predicted values yielded an expected cost and optimal placement configuration with
  lowest cost.predicted the completion time and price of a task based on priced timed
  Petri nets (PTPNs) to develop a resource allocation strategy to reduce latency and maximize re-
  source utilization in fog computing. PTPN effectively deals with the dynamic behavior of the fog
  system to generate the performance and time cost .
  Iterative methods over resources in the fog computing hierarchy is another effective technique.
   proposed a placement algorithm for hierarchical fog computing that exploits
  both conventional cloud and recent fog computing. The algorithm iterates from the fog towards
  the cloud for placing computation modules first on the available fog nodes. In this algorithm, a
  node is represented as a set of three attributes: the CPU, memory, and network bandwidth. Each
  computation module expresses its requirement in the form of the three attributes. The proposed
  solution first sorts the nodes and modules in ascending order to respectively associate the provided
  capacity with the requirement. The algorithm then places each module on an appropriate node
  that has enough resources, iterating from fog nodes to cloud nodes. The authors validated this
  algorithm using iFogSim, a fog computing simulation toolkit developed by Gupta et al..proposed service placement strategies for hierarchical Fog-to-Cloud (F2C)
  architectures in collaboration with service atomization and parallel execution. Service atomization
  divides a large service into smaller sub-services for workload distribution between the fog and
  the cloud. Parallel execution allows the divided sub-services to run on fog and cloud resources
  concurrently. Based on these techniques, the study suggests following three placement strategies:
  First-Fit (FF), Best-Fit (BF), and Best-fit with Queue (BQ). FF just selects available edge devices for
  allocation of sub-services, and if there are not available ones, FF sends the services to the cloud.
  BF sorts sub-services in ascending order based on the requested resources and allocates them to
  available edge devices. If the requested amount reaches a certain threshold for edge devices, the
  sub-services are sent to the cloud. BQ adopts BF as a basic strategy, but when edge devices are
  congested, it determines whether to send sub-services to the cloud or to queue them to the edge
  devices based on estimation.
  In contrast to the above iterative method, multiple iterations can be performed over the identi-
  fied problem space. Skarlat et al. proposed an approach called the Fog Service Placement Problem
  (FSPP) to optimally share resources in fog nodes among IoT services [159]. The FSPP considers QoS
  constraints such as latency or deadline requirements during placement. In the FSPP, a fog node is
  characterized by three attributes, the CPU, memory, and storage, similar to the work of Taneja et
  al. [166]. The FSPP suggests a proactive approach where the placement is performed periodically
  to meet the QoS requirement. When the response time of an application reaches the upper bound,
  the FSPP prioritizes the application and places it on a node that has enough resources. If there are
  not enough resources, the algorithm sends the service to the nearest fog network or cloud. The
  proposed model was evaluated on an extended iFogSim [60].</p>
  <button class="btn btn-secondary button " style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);" onclick="display('sectionarchi')">Previous</button>
  <button class="btn btn-danger button" style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);" onclick="display('sectionadvan')">next</button>
</div>
</div>
<div id="sectionadvan" class="modal">
 <div class="card4">

<header>
  <h1>Advantages of Fog and Edge Computing</h1>
</header>

<div id="advantages">
  <h2 class="h22">Advantages of Fog Computing:</h2>
  <ul class="ul">
      <li class="li">Low Latency: Fog computing reduces latency by processing data closer to the source, resulting in faster response times.</li>
      <li class="li">Bandwidth Efficiency: It reduces the amount of data that needs to be transmitted to the cloud, saving bandwidth.</li>
      <li class="li">Improved Privacy: Sensitive data can be processed locally, enhancing privacy and security.</li>
      <li class="li">Scalability: Fog nodes can be easily scaled to accommodate increasing workloads.</li>
      <li class="li">Reliability: Fog computing can continue to operate even when the connection to the cloud is lost.</li>
  </ul>

  <h2 class="h22">Advantages of Edge Computing:</h2>
  <ul class="ul">
      <li class="li">Ultra-Low Latency: Edge computing offers the lowest possible latency as it processes data at the edge of the network.</li>
      <li class="li">Real-time Processing: Critical applications that require real-time data processing benefit from edge computing.</li>
      <li class="li">Reduced Network Congestion: Edge devices process data locally, reducing network congestion and data transfer costs.</li>
      <li class="li">Privacy and Compliance: Edge computing ensures data privacy and compliance by processing sensitive data locally.</li>
      <li class="li">Autonomy: Edge devices can operate independently, even when disconnected from the network.</li>
  </ul>
</div>

</div>


<button class="btn btn-secondary button " style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);" onclick="display('sectionalg')">Previous</button>
<button class="btn btn-danger button" style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);" onclick="display('sectiondisa')">next</button>

</div>
<div id="sectiondisa" class="modal">
  <div class="card1">
  <header>
    <h1>Disadvantages of Fog and Edge Computing</h1>
</header>

<div id="advantages" >
    <h2 class="h22">Disadvantages of Fog Computing:</h2>
    <p class="li">1. Complexity: Implementing a fog computing infrastructure can be complex and require significant resources.</p>
    <p class="li">2. Cost: Setting up and maintaining fog nodes can be expensive, especially for small-scale applications.</p>
    <p class="li">3. Limited Scalability: Fog computing may have limitations in scaling to handle extremely large workloads.</p>
    <p class="li">4. Security Risks: The distributed nature of fog computing introduces potential security vulnerabilities.</p>
    <p class="li">5. Maintenance Challenges: Managing and maintaining a large number of fog nodes can be challenging.</p>

    <h2 class="h22">Disadvantages of Edge Computing:</h2>
    <p class="li">1. Limited Processing Power: Edge devices may have limited processing capabilities, which can constrain applications.</p>
    <p class="li">2. Data Storage Constraints: Edge devices may have limited storage capacity for storing data locally.</p>
    <p class="li">3. Network Dependence: Edge computing relies on network connectivity, and network issues can disrupt operations.</p>
    <p class="li">4. Initial Setup Complexity: Setting up edge computing infrastructure may require expertise and effort.</p>
    <p class="li">5. Data Synchronization: Ensuring data consistency and synchronization across edge devices can be challenging.</p>
</div>
</div>
<button class="btn btn-secondary button " style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);" onclick="display('sectionadvan')">Previous</button>
<button class="btn btn-danger button" style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);" onclick="display('sectionover')">next</button>
</div>
<div id="sectionover" class="modal">
  <div class="card3">
    <header>
      <h1>Methods to Overcome Disadvantages of Fog and Edge Computing</h1>
  </header>

  <div id="methods">
      <h2 class="h22">Overcoming Disadvantages of Fog Computing:</h2>
      <p class="p1">1. Simplified Management: Implement centralized management tools to simplify the deployment and maintenance of fog nodes.</p>
      <p class="p1">2. Cost Optimization: Explore cost-effective hardware solutions and consider cloud-fog hybrid models to reduce expenses.</p>
      <p class="p1">3. Scalability Planning: Design the fog architecture with scalability in mind, allowing for the addition of nodes as needed.</p>
      <p class="p1">4. Security Measures: Implement robust security protocols and regularly update security mechanisms to mitigate risks.</p>
      <p class="p1">5. Monitoring and Analytics: Utilize monitoring and analytics tools to detect and respond to performance and security issues.</p>

      <h2 class="h22">Overcoming Disadvantages of Edge Computing:</h2>
      <p class="p1">1. Edge Device Upgrades: Invest in more powerful edge devices to increase processing capabilities.</p>
      <p class="p1">2. Cloud Integration: Use cloud resources for data storage and offload processing when edge devices have limitations.</p>
      <p class="p1">3. Redundancy Planning: Implement redundancy and failover mechanisms to ensure continuous operation in case of network disruptions.</p>
      <p class="p1">4. Simplified Setup: Develop user-friendly setup and configuration tools to reduce the complexity of edge computing deployment.</p>
      <p class="p1">5. Data Synchronization Solutions: Implement robust data synchronization methods to maintain consistency across edge devices.</p>
  </div>





  </div>
  <button class="btn btn-secondary button " style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);" onclick="display('sectiondisa')">Previous</button>
  <button class="btn btn-danger button" style="box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.2);" onclick="display('sectionmain')">Finished</button>
</div>



</div>
    
    
</body>
<script>
    // Get the modal
    var modal = document.getElementById('id01');
    
    // When the user clicks anywhere outside of the modal, close it
    window.onclick = function(event) {
        if (event.target == modal) {
            modal.style.display = "none";
        }
    }
    
</script>

<script>
        // Get the modal
        var modal = document.getElementById('id02');
        
        // When the user clicks anywhere outside of the modal, close it
        window.onclick = function(event) {
            if (event.target == modal) {
                modal.style.display = "none";
            }
        }
</script>
<script>
  // Get the modal
  var modal = document.getElementByTagName('intro.html');
  
  // When the user clicks anywhere outside of the modal, close it
  window.onclick = function(event) {
      if (event.target == modal) {
          modal.style.display = "none";
      }
  }
</script>
<script>
          // Get the modal
          var modal = document.getElementById('id03');
          
          // When the user clicks anywhere outside of the modal, close it
          window.onclick = function(event) {
              if (event.target == modal) {
                  modal.style.display = "none";
              }
          }
          
</script>
<script>
            // Get the modal
            var modal = document.getElementById('sectionmainnn');
            
            // When the user clicks anywhere outside of the modal, close it
            window.onclick = function(event) {
                if (event.target == modal) {
                    modal.style.display = "none";
                }
            }
            function clickHandler() {
              stLight.options({
                  publisher:'0b-6eed-4740-81e7-aa3ee0bd9f85',
              });
              return false;
          }
          <More-sharebar><a href="#" onclick="javascript:clickHandler()">More options</a></More-sharebar>
            
</script>
<script type="text/javascript" src="https://d1tgh8fmlzexmh.cloudfront.net/ccbp-static-website/js/ccbp-ui-kit.js">
</script>
</html>